\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}The Challenge: A Risky Gridworld}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Our Approach: Learning from Experience}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The ``Cheat Sheet'' Analogy}{1}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}The Learning Algorithm}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Discussion of Findings}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Comparing Standard vs. High-Risk Scenarios}{2}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Case 1: Standard Penalty (-1)}{2}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The agent's learned strategy for the standard problem (penalty = -1).}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:policy_standard}{{1}{2}{The agent's learned strategy for the standard problem (penalty = -1)}{figure.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Case 2: High-Risk Penalty (-200)}{2}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The agent's highly cautious strategy with a -200 penalty.}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:policy_high_risk}{{2}{3}{The agent's highly cautious strategy with a -200 penalty}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The Crucial Role of Hyperparameters}{3}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A conceptual summary of how learning rate ($\alpha $), discount factor ($\gamma $), and the exploration schedule ($\epsilon $) impact Q-learning performance, with a table summarizing optimal ranges.}}{4}{figure.3}\protected@file@percent }
\newlabel{fig:hyperparameter_summary}{{3}{4}{A conceptual summary of how learning rate ($\alpha $), discount factor ($\gamma $), and the exploration schedule ($\epsilon $) impact Q-learning performance, with a table summarizing optimal ranges}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Analysis of the Learning Rate (alpha) on Q-value convergence and learning performance.}}{5}{figure.4}\protected@file@percent }
\newlabel{fig:hyperparameter_alpha}{{4}{5}{Analysis of the Learning Rate (alpha) on Q-value convergence and learning performance}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Analysis of the Discount Factor (gamma) on Q-value convergence and learning performance.}}{5}{figure.5}\protected@file@percent }
\newlabel{fig:hyperparameter_gamma}{{5}{5}{Analysis of the Discount Factor (gamma) on Q-value convergence and learning performance}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Analysis of the Epsilon Decay on Q-value convergence and learning performance.}}{5}{figure.6}\protected@file@percent }
\newlabel{fig:hyperparameter_epsilon}{{6}{5}{Analysis of the Epsilon Decay on Q-value convergence and learning performance}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces A heatmap showing the performance of different combinations of learning rate ($\alpha $) and discount factor ($\gamma $).}}{6}{figure.7}\protected@file@percent }
\newlabel{fig:hyperparameter_heatmap}{{7}{6}{A heatmap showing the performance of different combinations of learning rate ($\alpha $) and discount factor ($\gamma $)}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Learning efficiency for different learning rates.}}{7}{figure.8}\protected@file@percent }
\newlabel{fig:hyperparameter_efficiency}{{8}{7}{Learning efficiency for different learning rates}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Understanding Convergence: Policy vs. Q-Values}{7}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Convergence of Q-values (top) and the policy (bottom) over 100 episodes. The policy stabilizes quickly, while the Q-values continue to be refined.}}{7}{figure.9}\protected@file@percent }
\newlabel{fig:convergence}{{9}{7}{Convergence of Q-values (top) and the policy (bottom) over 100 episodes. The policy stabilizes quickly, while the Q-values continue to be refined}{figure.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Final Thoughts}{8}{section.4}\protected@file@percent }
\gdef \@abspage@last{8}
