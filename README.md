# Certificated Actor-Critic (CAC) for Safe Robot Navigation

## Overview

Implementation of the **Certificated Actor-Critic (CAC)** algorithm from the paper:
> "Certificated Actor-Critic: Hierarchical Reinforcement Learning with Control Barrier Functions for Safe Navigation"  
> Xie, Junjun and Zhao, Shuhao and Hu, Liang and Gao, Huijun  
> arXiv preprint arXiv:2501.17424 (2025)

This is a **model-free reinforcement learning framework** that achieves **safe goal-reaching navigation** through a two-stage hierarchical learning process:

1. **Stage 1: Safety Critic Construction** - Learn collision-free navigation using Control Barrier Function (CBF)-derived rewards
2. **Stage 2: Restricted Policy Update** - Improve goal-reaching performance while maintaining safety guarantees through restricted gradient updates

## üéØ Key Features

‚úÖ **Safety Guarantees** - CBF-based forward invariance ensures collision avoidance  
‚úÖ **Safety Certificates** - Quantitative safety evaluation via learned critics  
‚úÖ **Hierarchical Learning** - Separate stages for safety and goal-reaching  
‚úÖ **Restricted Gradients** - Maintains safety during policy improvement  
‚úÖ **Model-Free** - No explicit system model required  
‚úÖ **Realistic Physics** - High-fidelity vehicle dynamics simulation  
‚úÖ **Dynamic Obstacles** - Handles both static and moving obstacles

## üìÅ Project Structure

```
.
‚îú‚îÄ‚îÄ README.md                    # This file
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ pyproject.toml              # Project configuration
‚îÇ
‚îú‚îÄ‚îÄ main.py                     # Q-Learning GridWorld (legacy)
‚îú‚îÄ‚îÄ train_cac.py                # Main CAC training script (Algorithm 1)
‚îú‚îÄ‚îÄ goal_reaching_test.py       # Improved training with optimizations
‚îú‚îÄ‚îÄ demo_cac.py                 # Interactive demo with moving obstacles
‚îú‚îÄ‚îÄ device_config.py            # Device configuration (CPU/GPU/MPS)
‚îÇ
‚îú‚îÄ‚îÄ realistic_car_env.py        # Realistic car physics environment
‚îú‚îÄ‚îÄ scenarios.py                # Environment wrappers and scenarios
‚îÇ
‚îú‚îÄ‚îÄ stage1/                     # Stage 1: Safety Critic Construction
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ agent.py                # SAC agent implementation
‚îÇ   ‚îú‚îÄ‚îÄ cbf.py                  # Control Barrier Function (CBF)
‚îÇ   ‚îú‚îÄ‚îÄ checkpoints.py          # Model saving/loading
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Configuration classes
‚îÇ   ‚îú‚îÄ‚îÄ env_setup.py            # Environment setup utilities
‚îÇ   ‚îú‚îÄ‚îÄ eval.py                 # Evaluation utilities
‚îÇ   ‚îú‚îÄ‚îÄ networks.py             # Neural network architectures
‚îÇ   ‚îú‚îÄ‚îÄ training.py             # Training utilities
‚îÇ   ‚îú‚îÄ‚îÄ viz.py                  # Visualization tools
‚îÇ   ‚îî‚îÄ‚îÄ wrappers.py             # Environment wrappers
‚îÇ
‚îú‚îÄ‚îÄ stage2/                     # Stage 2: Restricted Policy Update
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ agent.py                # Stage 2 agent
‚îÇ   ‚îú‚îÄ‚îÄ checkpoints.py          # Checkpoint management
‚îÇ   ‚îú‚îÄ‚îÄ eval.py                 # Evaluation tools
‚îÇ   ‚îú‚îÄ‚îÄ goal_clf.py             # Control Lyapunov Function (CLF)
‚îÇ   ‚îú‚îÄ‚îÄ replay.py               # Replay buffer
‚îÇ   ‚îî‚îÄ‚îÄ training.py             # Training utilities
‚îÇ
‚îú‚îÄ‚îÄ checkpoints/                # Pre-trained models
‚îÇ   ‚îú‚îÄ‚îÄ responsive_stage1.pt    # Stage 1 trained policy
‚îÇ   ‚îú‚îÄ‚îÄ responsive_stage2.pt    # Stage 2 trained policy
‚îÇ   ‚îú‚îÄ‚îÄ stage1_safe_policy.pt   # Alternative Stage 1 checkpoint
‚îÇ   ‚îî‚îÄ‚îÄ stage2_final_policy.pt  # Alternative Stage 2 checkpoint
‚îÇ
‚îú‚îÄ‚îÄ Documents/                  # Research papers
‚îÇ   ‚îî‚îÄ‚îÄ 2501.17424v1.pdf       # CAC paper
‚îÇ
‚îú‚îÄ‚îÄ images/                     # Visualization outputs
‚îî‚îÄ‚îÄ Report/                     # Project reports
```

## üß† Algorithm Overview

### Control Barrier Function (CBF)

The CBF `h(s)` defines a safe set `C = {s : h(s) ‚â• 0}`. For safety:

```
h(s_{t+1}) + (Œ±‚ÇÄ - 1)h(s_t) ‚â• 0
```

### Stage 1: Safety Critic Construction

**Objective**: Learn a safe policy `œÄ*_safe` using CBF-derived rewards

**Safety Reward** (Equations 7 & 12 from paper):
```python
r‚ÇÅ(s_t, a_t) = exp(min(h(s_{t+1}) + (Œ±‚ÇÄ - 1)h(s_t), 0))
```

**Output**: Safe policy + Safety critics `V^œÄ‚ÇÅ`, `Q^œÄ‚ÇÅ`

The safety critics serve as **safety certificates**:
- If `V^œÄ‚ÇÅ(s‚ÇÄ) ‚âà 1.0`, the system is safe from state `s‚ÇÄ`
- Can compare relative safety between different policies

### Stage 2: Restricted Policy Update  

**Objective**: Improve goal-reaching while maintaining safety

**Navigation Reward** (Equation 9 from paper):
```python
r‚ÇÇ(s_t, a_t) = -max(l(s_{t+1}) + (Œ≤‚ÇÄ - 1)l(s_t), 0)
```
where `l(s)` is a Control Lyapunov Function (CLF), e.g., squared distance to goal.

**Restricted Gradient Update** (Equation 10 from paper):
```
‚àáŒ∏ = argmax_e e¬∑‚àáŒ∏J‚ÇÇ(Œ∏)
s.t. e¬∑‚àáŒ∏J‚ÇÅ(Œ∏) ‚â• 0, ||e|| ‚â§ ||‚àáŒ∏J‚ÇÇ(Œ∏)||
```

This ensures the safety critic doesn't decrease while improving goal-reaching performance.

## üöÄ Quick Start

### Installation

1. **Clone the repository**:
```bash
git clone https://github.com/Jeevan-HM/RL-in-Robotics.git
cd RL-in-Robotics
```

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

Or use UV for faster installation:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh  # Install UV
uv sync  # Install dependencies
```

### Training CAC Agent

#### Option 1: Train Both Stages Sequentially
```bash
# Train complete CAC pipeline (Stage 1 + Stage 2)
python train_cac.py --steps-stage1 250000 --steps-stage2 250000
```

#### Option 2: Train Stages Separately
```bash
# Stage 1 only: Learn safe policy
python train_cac.py --stage 1 --steps-stage1 250000

# Stage 2 only: Improve goal-reaching (requires Stage 1 checkpoint)
python train_cac.py --stage 2 --steps-stage2 250000
```

#### Advanced Training Options
```bash
python train_cac.py \
    --steps-stage1 500000 \
    --steps-stage2 500000 \
    --alpha0 0.2 \           # CBF decay rate (lower = stricter safety)
    --beta0 0.9 \            # CLF decay rate (lower = faster goal approach)
    --device cuda \          # Use GPU (auto/cpu/cuda/mps)
    --checkpoint-dir ./my_checkpoints
```

#### Improved Training with Optimizations
```bash
# Use the improved training script with better hyperparameters
python goal_reaching_test.py \
    --stage 0 \              # Train both stages
    --steps-stage1 250000 \
    --steps-stage2 250000 \
    --difficulty medium \    # easy/medium/hard
    --device auto
```

### Evaluation

Evaluation functions are available in the stage1 and stage2 modules:

```python
# In your code, use the evaluation functions:
from stage1.eval import evaluate_stage1
from stage2.eval import evaluate_stage2

# Evaluate Stage 1 safety policy
stage1_results = evaluate_stage1(env, agent, episodes=100)

# Evaluate Stage 2 goal-reaching policy
stage2_results = evaluate_stage2(env, agent, goal_clf, cbf, episodes=100)
```

### Interactive Demo

```bash
# Visualize trained agent with moving obstacles
python demo_cac.py \
    --checkpoint checkpoints/responsive_stage2.pt \
    --episodes 3 \
    --moving-obstacles 3 \
    --obstacle-speed 3.0
```

## üèóÔ∏è Environment Features

### Realistic Car Physics

The environment simulates a realistic vehicle using a bicycle model with:

- **Vehicle Dynamics**: Mass (800kg), inertia (1000 kg‚ãÖm¬≤), wheelbase (2.7m)
- **Tire Physics**: Friction coefficient (0.9), rolling resistance (0.01)
- **Aerodynamics**: Drag coefficient (0.25), frontal area (2.0 m¬≤)
- **Engine/Brake**: Max engine force (8000N), max brake force (12000N)
- **Steering**: Max angle (50¬∞), max steering rate (90¬∞/s)

### Sensors

- **LIDAR**: 32-ray distance sensor with 25m range and 270¬∞ FOV
- **State Information**: Position, velocity, heading, acceleration
- **Safety Metrics**: Distance to nearest obstacle, collision detection

### Obstacles

- **Static Obstacles**: Fixed circular obstacles with configurable sizes
- **Dynamic Obstacles**: Moving obstacles with realistic velocities (up to 5 m/s)
- **Walls**: Boundary walls to constrain navigation space

### Goals

- **Goal Radius**: 3.0m (configurable)
- **CLF-based Rewards**: Encourage efficient goal approach
- **Waypoints**: Optional intermediate goals (future feature)

## üéì Implementation Details

### Neural Networks

- **Actor (Policy)**: 2-layer MLP with 256 units per layer
- **Critics**: 2 twin Q-networks, 3-layer MLP with 256 units each
- **Activation**: ReLU for hidden layers, Tanh for policy output
- **Output**: Continuous actions (steering, throttle/brake)

### Soft Actor-Critic (SAC) Algorithm

- **Optimization**: Adam optimizer with learning rate 3√ó10‚Åª‚Å¥
- **Discount Factor**: Œ≥ = 0.99
- **Target Network Update**: Soft update with œÑ = 0.005
- **Entropy Tuning**: Automatic temperature adjustment
- **Replay Buffer**: 500k transitions

### Training Configuration

- **Batch Size**: 256
- **Initial Exploration**: 5000-10000 random steps
- **Update Frequency**: Every 50-100 steps
- **Episode Length**: Up to 3000 steps (5 minutes at 0.1s timestep)

## üìä Expected Results

### Stage 1: Safety Critic Construction

After ~250k steps (approximately 2-4 hours on CPU):
- **Safe Rate**: >95% collision-free episodes
- **Safety Critic**: `V^œÄ‚ÇÅ(s) ‚âà 1.0` for safe states
- **Behavior**: Conservative, collision-avoiding navigation
- **Goal Rate**: ~20-40% (safety prioritized over goal-reaching)

### Stage 2: Restricted Policy Update  

After additional ~250k steps:
- **Safe Rate**: Maintained >95% (safety preserved)
- **Goal Rate**: >80% goal-reaching success
- **Behavior**: Efficient goal-reaching while staying safe
- **Navigation**: Smooth, human-like trajectories

### Dynamic Obstacle Performance

With 3 moving obstacles at 3 m/s:
- **Collision Avoidance**: >90% success
- **Close Calls**: <5 per episode (within 5m of moving obstacle)
- **Goal Success**: >70% in dynamic environments

## üõ†Ô∏è Troubleshooting

### Import Errors

If you get module import errors:
```bash
# Ensure you're in the project root directory
cd /path/to/RL-in-Robotics

# Install all dependencies
pip install -r requirements.txt
```

### Training Issues

**Low safe rate in Stage 1**:
- Increase `--steps-stage1` to 500k or more
- Adjust `--alpha0` to a lower value (e.g., 0.1) for stricter safety

**Poor goal-reaching in Stage 2**:
- Increase `--steps-stage2` to 500k or more
- Adjust `--beta0` to a lower value (e.g., 0.8) for more aggressive goal approach
- Use the improved training script: `goal_reaching_test.py`

**Memory issues**:
- Reduce replay buffer capacity in code (edit `agent.py`)
- Use CPU instead of GPU: `--device cpu`

### GPU/Device Issues

```bash
# Force CPU
python train_cac.py --device cpu

# Force CUDA (NVIDIA GPU)
python train_cac.py --device cuda

# Force Apple Silicon (MPS)
python train_cac.py --device mps

# Auto-detect best device
python train_cac.py --device auto
```

### Visualization Issues

If PyQt5 rendering doesn't work:
```bash
# Try installing PyQt5 separately
pip install pyqt5 --force-reinstall

# Or use matplotlib backend
pip install matplotlib pyqt5
```

## üìö Legacy Q-Learning GridWorld

This repository also contains a classical Q-learning implementation for educational purposes:

```bash
# Run Q-learning on GridWorld with hyperparameter analysis
python main.py --episodes 10000
```

Features:
- Stochastic GridWorld environment (4x3 grid)
- Comprehensive hyperparameter analysis (Œ±, Œ≥, Œµ decay)
- Rich visualizations of convergence and Q-table heatmaps
- Educational walkthrough of RL fundamentals

See `main.py` for the complete implementation.

## üî¨ Research Context

This implementation is based on the research paper:

```bibtex
@article{xie2025certificated,
  title={Certificated Actor-Critic: Hierarchical Reinforcement Learning 
         with Control Barrier Functions for Safe Navigation},
  author={Xie, Junjun and Zhao, Shuhao and Hu, Liang and Gao, Huijun},
  journal={arXiv preprint arXiv:2501.17424},
  year={2025}
}
```

**Paper available in**: `Documents/2501.17424v1.pdf`

### Key Contributions from Paper

1. **Hierarchical Safety Framework**: Separates safety learning from goal-reaching
2. **Safety Certificates**: Uses critic networks as quantitative safety measures
3. **Restricted Gradients**: Novel gradient constraint preserves safety during policy updates
4. **Model-Free Approach**: No explicit dynamics model required
5. **Theoretical Guarantees**: Provable safety under CBF conditions

## üîÆ Future Improvements

The current implementation provides a solid foundation. Potential enhancements:

1. **Full Restricted Gradient Implementation**: Implement exact Equation 10 optimization with constrained gradient projection
2. **Dual Critics**: Separate safety critics (`V^œÄ‚ÇÅ`, `Q^œÄ‚ÇÅ`) and navigation critics (`V^œÄ‚ÇÇ`, `Q^œÄ‚ÇÇ`)
3. **High-Order CBFs**: Smoother control using second-order or adaptive CBFs
4. **Multi-Agent Extension**: Coordinate multiple robots safely
5. **Real Robot Deployment**: Port to physical platforms (ROS integration)
6. **Curriculum Learning**: Progressive difficulty increase during training
7. **Attention Mechanisms**: Better obstacle tracking for dynamic environments

## üìñ Additional Resources

### Learning Materials

- **Control Barrier Functions**: [Ames et al., 2019 - "Control Barrier Functions: Theory and Applications"](https://ieeexplore.ieee.org/document/8796030)
- **Soft Actor-Critic**: [Haarnoja et al., 2018 - "Soft Actor-Critic Algorithms and Applications"](https://arxiv.org/abs/1812.05905)
- **Safe RL Survey**: [Garc√≠a & Fern√°ndez, 2015 - "A Comprehensive Survey on Safe Reinforcement Learning"](https://jmlr.org/papers/v16/garcia15a.html)

### Related Projects

- **safety-gym**: OpenAI's safe RL benchmark environments
- **safe-control-gym**: Benchmark for safe learning-based control
- **CBF-QP**: Real-time CBF-based quadratic programming controllers

## üìÑ License

This implementation is for research and educational purposes. Please cite the original paper if you use this code in your research.

## üôè Acknowledgments

- Original paper authors: Junjun Xie, Shuhao Zhao, Liang Hu, Huijun Gao
- Soft Actor-Critic implementation inspired by [Spinning Up](https://spinningup.openai.com/)
- Physics simulation based on realistic vehicle dynamics models

## üìß Contact

For questions about the implementation:
- Open an issue on GitHub: [https://github.com/Jeevan-HM/RL-in-Robotics/issues](https://github.com/Jeevan-HM/RL-in-Robotics/issues)

For questions about the original paper:
- Contact the authors (see paper for details)

---

**Built with** ‚ù§Ô∏è **for safe and intelligent robot navigation**
