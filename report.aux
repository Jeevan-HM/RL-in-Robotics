\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}The Challenge: A Risky Gridworld}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Our Approach: Learning from Experience}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The ``Cheat Sheet'' Analogy}{1}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}The Learning Algorithm}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Discussion of Findings}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Crucial Role of Hyperparameters}{2}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Conceptual overview of hyperparameter effects on Q-learning performance. Learning rate ($\alpha $) controls update speed, discount factor ($\gamma $) determines farsightedness, and $\epsilon $ decay balances exploration vs exploitation.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:hyperparameter_summary}{{1}{3}{Conceptual overview of hyperparameter effects on Q-learning performance. Learning rate ($\alpha $) controls update speed, discount factor ($\gamma $) determines farsightedness, and $\epsilon $ decay balances exploration vs exploitation}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Learning rate ($\alpha $) analysis showing that moderate values (0.1-0.3) achieve optimal balance between convergence speed and stability. High values cause instability while low values learn too slowly.}}{4}{figure.2}\protected@file@percent }
\newlabel{fig:hyperparameter_alpha}{{2}{4}{Learning rate ($\alpha $) analysis showing that moderate values (0.1-0.3) achieve optimal balance between convergence speed and stability. High values cause instability while low values learn too slowly}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Discount factor ($\gamma $) analysis showing that high values ($\gamma \geq 0.9$) are essential for optimal performance. Low values cause myopic behavior that prioritizes immediate rewards over the distant goal.}}{5}{figure.3}\protected@file@percent }
\newlabel{fig:hyperparameter_gamma}{{3}{5}{Discount factor ($\gamma $) analysis showing that high values ($\gamma \geq 0.9$) are essential for optimal performance. Low values cause myopic behavior that prioritizes immediate rewards over the distant goal}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Exploration schedule ($\epsilon $ decay) analysis revealing that moderate decay rates (0.999) optimally balance exploration and exploitation. Fast decay leads to premature convergence, slow decay wastes episodes on random actions.}}{5}{figure.4}\protected@file@percent }
\newlabel{fig:hyperparameter_epsilon}{{4}{5}{Exploration schedule ($\epsilon $ decay) analysis revealing that moderate decay rates (0.999) optimally balance exploration and exploitation. Fast decay leads to premature convergence, slow decay wastes episodes on random actions}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Performance heatmap showing optimal combinations require high $\gamma $ (long-term planning) and moderate $\alpha $ (stable learning). The sharp performance drop at low $\gamma $ demonstrates its critical importance.}}{6}{figure.5}\protected@file@percent }
\newlabel{fig:hyperparameter_heatmap}{{5}{6}{Performance heatmap showing optimal combinations require high $\gamma $ (long-term planning) and moderate $\alpha $ (stable learning). The sharp performance drop at low $\gamma $ demonstrates its critical importance}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Learning efficiency showing episode length evolution. Higher learning rates achieve rapid initial improvement but remain volatile, while moderate rates provide steady convergence to optimal path lengths.}}{6}{figure.6}\protected@file@percent }
\newlabel{fig:hyperparameter_efficiency}{{6}{6}{Learning efficiency showing episode length evolution. Higher learning rates achieve rapid initial improvement but remain volatile, while moderate rates provide steady convergence to optimal path lengths}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Understanding Convergence: Policy vs. Q-Values}{6}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Convergence analysis showing that policy stabilizes around episode 40 while Q-values continue fluctuating. This demonstrates that optimal decision-making emerges before complete value convergence.}}{7}{figure.7}\protected@file@percent }
\newlabel{fig:convergence}{{7}{7}{Convergence analysis showing that policy stabilizes around episode 40 while Q-values continue fluctuating. This demonstrates that optimal decision-making emerges before complete value convergence}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Comparing Standard vs. High-Risk Scenarios}{7}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Case 1: Standard Penalty (-1)}{7}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The agent's learned strategy for the standard problem (penalty = -1).}}{7}{figure.8}\protected@file@percent }
\newlabel{fig:policy_standard}{{8}{7}{The agent's learned strategy for the standard problem (penalty = -1)}{figure.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Case 2: High-Risk Penalty (-200)}{7}{subsubsection.3.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The agent's highly cautious strategy with a -200 penalty.}}{8}{figure.9}\protected@file@percent }
\newlabel{fig:policy_high_risk}{{9}{8}{The agent's highly cautious strategy with a -200 penalty}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Understanding the Q-Table: The Agent's Internal Knowledge}{8}{subsection.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Q-table heatmap showing learned Q-values for each action from every state. The visualization reveals how the agent learned to value actions based on their expected future rewards, with clear patterns of goal attraction and penalty avoidance.}}{9}{figure.10}\protected@file@percent }
\newlabel{fig:q_table_heatmap}{{10}{9}{Q-table heatmap showing learned Q-values for each action from every state. The visualization reveals how the agent learned to value actions based on their expected future rewards, with clear patterns of goal attraction and penalty avoidance}{figure.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Final Thoughts}{9}{section.4}\protected@file@percent }
\gdef \@abspage@last{9}
